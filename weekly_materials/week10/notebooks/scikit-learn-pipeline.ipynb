{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "\n",
    "As you have been learning for the past few weeks, a typical machine learning task consists of several data preprocessing steps ranging from imputing missing values to creating new features. The tasks are iterative i.e. we need to run the same or slightly modified steps multiple times to understand their efficacy. At the same time, we might need to compare wide ranging algorithms for understanding their efficacy in building accurate models on preprocessed data. Scikit-learn helps automate such workflow of repetitive tasks with Pipeline estimator.\n",
    "\n",
    "Apart from automating the workflow, Pipeline provides another significant value. One of the things that we need to be wary of in machine learning is the possibility of leaking data from training to dev and test dataset. One common way of letting down our guards against leaking is during data preprocessing such as when by applying data scaling or normalization on entire training dataset that would be further split into different folds of train and dev dataset for model building and hyperparameter tuning. Chaining the preprocessing and model building steps into a linear sequence via pipeline can help you easily avoid such serious mistakes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Consider a machine learning workflow with following tasks:\n",
    "\n",
    "- Imputing missing values\n",
    "- Creating polynomial features\n",
    "- Applying feature scaling\n",
    "- Fitting a classification model\n",
    "\n",
    "We will combine the above four tasks into a pipeline to build a classification model against the Iris dataset. We will use the pipeline for two classification algorithms: logistic regression and random forest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.30, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a trasformer instance \n",
    "\n",
    "\n",
    "ColumnTransformer enable separate transformations of different columns or subset of columns. Create a transformer that does mean imputing for first and second columns, and generate polynomial features using third and fourth columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "transformer = ColumnTransformer([(\"norm1\", SimpleImputer(missing_values=np.nan, strategy='mean'), [0, 1]),\n",
    "                                ('poly', PolynomialFeatures(2),[2,3])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now chain the transformer together with scaler and logistic regression transform into a pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "logistic_model=Pipeline(steps=([('transformer', transformer),\n",
    "                      ('scaler',StandardScaler()), ('LR', LogisticRegression())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "rf_model = Pipeline(steps=([('transformer', transformer),\n",
    "                      ('scaler',StandardScaler()), ('LR', ensemble.RandomForestClassifier())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also include your custom transformer using FunctionTransfomer and in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_as_it_is(X):\n",
    "    return X*1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "rf_model = Pipeline(steps=([('transformer', transformer),('do_nothing',FunctionTransformer(return_as_it_is)),\n",
    "                      ('scaler',StandardScaler()), ('LR', ensemble.RandomForestClassifier())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally fit the two pipeline estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = logistic_model.fit(X_train,y_train)\n",
    "\n",
    "rf = rf_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = rf_model.predict(X_test);\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logistic_model.predict(X_test);\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [here](https://stackoverflow.com/questions/40708077/what-is-the-difference-between-pipeline-and-make-pipeline-in-scikit) to learn the difference between `pipeline` and `make_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
