{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "\n",
    "As you have been learning for the past few weeks, a typical machine learning task consists of a varying degree \n",
    "of data preprocessing steps ranging from imputing missing values to creating new features. The tasks are iterative i.e. we need to run the same or slightly modified preprocessing steps multiple times to understand their efficacy. At the same time, we might need to compare wide ranging algorithms for their utility in building models on preprocessed data. Scikit-learn helps automate such workflow of repetitive tasks with Pipeline.\n",
    "\n",
    "Apart from automating the workflow, Pipeline provides another significant value. One of the things that we need to be wary of in machine learning is the possibility of leaking data from training to dev and test dataset. One common way of letting down our guards against leaking is during data preprocessing such as when by applying data scaling or normalization on entire training dataset that would be further split into different folds of train and dev dataset for model building and hyperparameter tuning. Chaining a string of preprocessing and model building steps via pipeline can help you easily avoid such mistakes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Consider a machine learning workflow with following tasks:\n",
    "\n",
    "- Imputing missing values\n",
    "- Creating polynomial features\n",
    "- Applying feature scaling\n",
    "- Fitting a classification model\n",
    "\n",
    "We will combine the above four tasks into a pipeline to build a classification model against the Iris dataset. We will use the pipeline for two classification algorithms: logistic regression and random forest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into trianing and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.30, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a trasformer instance \n",
    "\n",
    "\n",
    "ColumnTransformer enable separate transformations of different columns or subset of columns. Create a transformer that does mean imputing for first and second columns, and generate polynomial features using third and fourth columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "transformer = ColumnTransformer([(\"norm1\", SimpleImputer(missing_values=np.nan, strategy='mean'), [0, 1]),\n",
    "                                ('poly', PolynomialFeatures(2),[2,3])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now chain the transformer together with scaler and logistic regression transform into a pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "logistic_model=Pipeline(steps=([('transformer', transformer),\n",
    "                      ('scaler',StandardScaler()), ('LR', LogisticRegression())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "rf_model = Pipeline(steps=([('transformer', transformer),\n",
    "                      ('scaler',StandardScaler()), ('LR', ensemble.RandomForestClassifier())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finally fit the two pipeline estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = logistic_model.fit(X_train,y_train)\n",
    "\n",
    "rf = rf_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = lr.predict(X_test);\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf.predict(X_test);\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [here](https://stackoverflow.com/questions/40708077/what-is-the-difference-between-pipeline-and-make-pipeline-in-scikit) to learn the difference between `pipeline` and `make_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
